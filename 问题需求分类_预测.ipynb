{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Loss = []\n",
    "Acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zhihu = pd.read_table(r'知乎问题数据集.txt',encoding='utf-8',sep=',',header=None,error_bad_lines=False)\n",
    "zhihu = pd.read_csv(r'E:\\论文\\数据\\知乎.csv',encoding='utf-8',sep=',',header=0,error_bad_lines=False)\n",
    "# zhihu.dropna(inplace=True)\n",
    "# print('Existence：'+str(len(zhihu[zhihu[2]==0.0])))\n",
    "# print('Growth:'+str(len(zhihu[zhihu[2]==1.0])))\n",
    "# print('Relatedness:'+str(len(zhihu[zhihu[2]==2.0])))\n",
    "\n",
    "zhihu.drop_duplicates(subset=['标题'], keep='first', inplace=True)\n",
    "len(zhihu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "seed_words1 = pd.read_excel(r'E:\\论文\\数据\\WenXinV4.0师姐发的版本\\dic\\dictionary_all.xlsx',header=0,index_col=None)\n",
    "seed_words1\n",
    "seed_words = seed_words1['words'].to_list()\n",
    "seed_words\n",
    "jieba.load_userdict(r'E:\\pythonProject\\需求分析词典\\Seed_words.txt')\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#遍历知乎数据集，给知乎数据集分词，以量化文本\n",
    "# for i in range(len(zhihu)):\n",
    "#     zhihu_cut = tokenize_zh(zhihu.iloc[i,0])\n",
    "#     print(zhihu_cut)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stopchineseword(segResult):\n",
    "    file = open(\"f:\\\\chineseStopWords.txt\",\"r\")\n",
    "    data = dict()\n",
    "    new_segResult=[]\n",
    "    for i in file.readlines(): #从文件中读取数据并将其添加到列表中\n",
    "        data.append(i.strip())\n",
    "    for i in segResult:\n",
    "        if i in data:  #比较是否为停用词\n",
    "            continue\n",
    "        else:\n",
    "            new_segResult.append(i)\n",
    "    return new_segResult"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Existence_dictionary = dict()\n",
    "Growth_dictionary = dict()\n",
    "Relatedness_dictionary = dict()\n",
    "Existence = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\Result\\Existence_words_score_ultimate.txt',encoding='utf-8',header=None,sep=',')\n",
    "# Existence\n",
    "for i in range(len(Existence)):\n",
    "    # print(Existence.iloc[i,0])\n",
    "    Existence_dictionary[Existence.iloc[i,0]] = float(Existence.iloc[i,1])\n",
    "# print(Existence_dictionary)\n",
    "Growth = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\Result\\Growth_words_score_ultimate.txt',encoding='utf-8',header=None,sep=',')\n",
    "# Growth\n",
    "for i in range(len(Growth)):\n",
    "    Growth_dictionary[Growth.iloc[i,0]] = float(Growth.iloc[i,1])\n",
    "# print(Growth_dictionary)\n",
    "Relatedness = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\Result\\Relatedness_words_score_ultimate.txt',encoding='utf-8',header=None,sep = ',')\n",
    "for i in range(len(Relatedness)):\n",
    "    Relatedness_dictionary[Relatedness.iloc[i,0]] = float(Relatedness.iloc[i,1])\n",
    "# print((Relatedness_dictionary.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Relatedness_dictionary['自护力'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "score=[]\n",
    "count=0\n",
    "start_time = datetime.datetime.now()\n",
    "for i in range(len(zhihu)):\n",
    "    Existence_cut = tokenize_zh(str(zhihu.iloc[i,1]))\n",
    "        # print(Existence_cut)\n",
    "    label = -1\n",
    "    Existence_score=0\n",
    "    Growth_score=0\n",
    "    Relatedness_score=0\n",
    "    for j1 in Existence_cut:\n",
    "        for k1 in Existence_dictionary.keys():\n",
    "            if(j1 == k1):\n",
    "                Existence_score += Existence_dictionary[j1]\n",
    "            else:\n",
    "                continue\n",
    "    for j2 in Existence_cut:\n",
    "        for k2 in Growth_dictionary.keys():\n",
    "            if(j2 == k2):\n",
    "                Growth_score += Growth_dictionary[k2]\n",
    "            else:\n",
    "                continue\n",
    "    for j3 in Existence_cut:\n",
    "        for k3 in Relatedness_dictionary.keys():\n",
    "            if(j3 == k3):\n",
    "                Relatedness_score += Relatedness_dictionary[k3]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    if (Existence_score >= Growth_score) & (Existence_score >= Relatedness_score):\n",
    "        label = 0\n",
    "    elif (Growth_score >= Existence_score) & (Growth_score >= Relatedness_score):\n",
    "        label = 1\n",
    "    elif (Relatedness_score >= Existence_score) & (Relatedness_score >= Growth_score):\n",
    "        label = 2\n",
    "    with open(r'zhihu_label_unique2.txt',encoding='utf=8',mode='a+') as f:\n",
    "        f.write(str(label)+'\\n')\n",
    "        print(count)\n",
    "        count+=1\n",
    "    # print('Label:'+str(label))\n",
    "end_time = datetime.datetime.now()\n",
    "print(end_time-start_time)\n",
    "    # score.append(label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label = pd.read_table(r'E:\\pythonProject\\需求分析词典\\zhihu_label_unique_all.txt',encoding='utf-8',header=None)\n",
    "label.columns=['label']\n",
    "# label[label['label']==0].count()\n",
    "# label[label['label']==1].count()\n",
    "# label[label['label']==2].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(zhihu).to_csv(r'zhihu_unique.csv',encoding='utf-8',header=0,index=0)\n",
    "zhihu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zhihu=pd.read_csv(r'zhihu_unique.csv',encoding='utf-8',header=None)\n",
    "zhihu = pd.concat([zhihu,label],axis=1)\n",
    "zhihu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zhihu[zhihu['label']==0].shape\n",
    "# zhihu[zhihu['label']==0][0]\n",
    "zhihu[zhihu['label']==1][0]\n",
    "# zhihu[zhihu['label']==2][0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(zhihu[zhihu['label']==0]).to_csv(r'知乎_unique_0.txt',encoding='utf-8',header=0,sep=',')\n",
    "zhihu_0 = pd.read_table(r'知乎_unique_0.txt',encoding='utf-8',header=None,index_col=0,sep=',')\n",
    "zhihu_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(zhihu[zhihu['label']==1]).to_csv(r'知乎_unique_1.txt',encoding='utf-8',header=0,sep=',')\n",
    "zhihu_1 = pd.read_table(r'知乎_unique_1.txt',encoding='utf-8',header=None,index_col=0,sep=',')\n",
    "zhihu_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zhihu[zhihu['label']==2].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(zhihu[zhihu['label']==2]).to_csv(r'知乎_unique_2.txt',encoding='utf-8',header=0,sep=',')\n",
    "zhihu_2 = pd.read_table(r'知乎_unique_2.txt',encoding='utf-8',header=None,index_col=0,sep=',')\n",
    "zhihu_2[1]\n",
    "pd.DataFrame(zhihu_2[1]).to_csv(r'zhihu_1_unique_2.txt',encoding='utf-8',header=None,index=0,sep=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictionary = pd.read_table(r'E:\\pythonProject\\需求分析词典\\dictionary.txt',encoding='utf-8',header=None)\n",
    "# dictionary = dictionary[0].tolist()\n",
    "dictionary=dictionary[0].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jieba.load_userdict(dictionary)\n",
    "def tokenize_zh(text):\n",
    "    seg = jieba.lcut(text)\n",
    "    words = ' '.join(seg)\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 慎点\n",
    "# jieba.load_userdict(dictionary)\n",
    "count = 0\n",
    "with open(r'知乎_2_分词结果.txt',encoding='utf-8',mode='w+') as f:\n",
    "    for i in zhihu_1[1]:\n",
    "        # print(i)\n",
    "        cut_text = tokenize_zh(i)\n",
    "        # print(cut_text)\n",
    "        f.write(str(cut_text))\n",
    "        f.write('\\n')\n",
    "        print(count)\n",
    "        count+=1\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zhihu_1_fenci=pd.read_table(r'知乎_1_分词结果.txt',encoding='utf-8',header=None)\n",
    "zhihu_2_fenci=pd.read_table(r'知乎_2_分词结果.txt',encoding='utf-8',header=None)\n",
    "# zhihu_1_fenci\n",
    "zhihu_2_fenci"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba\n",
    "import sys\n",
    "from wordcloud import WordCloud\n",
    "# path=r'知乎_1.txt'\n",
    "# path=r'知乎_2.txt'\n",
    "path=r'zhihu_1_unique_2.txt'\n",
    "# path=r'知乎_unique_2.txt'\n",
    "import matplotlib.pyplot as plt\n",
    "text = open(file=path,encoding='utf-8').read()\n",
    "jieba.load_userdict(r'E:\\pythonProject\\需求分析词典\\dictionary.txt')\n",
    "wordlist=jieba.cut(text,cut_all=True)\n",
    "wl_space_split=\" \".join(wordlist)\n",
    "\n",
    "print(wl_space_split)\n",
    "# my_wordcloud=WordCloud().generate(wl_space_split)\n",
    "# plt.imshow(my_wordcloud)\n",
    "# plt.axes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv(r'E:\\论文\\python代码\\停用词表\\stopwords_all2.txt',encoding='utf-8',header=None,error_bad_lines=False,quoting=3)\n",
    "stopwords\n",
    "for i in stopwords[0]:\n",
    "    # print(type(i))\n",
    "    if i in wl_space_split:\n",
    "        wl_space_split=wl_space_split.replace(i,'')\n",
    "        # wl_space_split=wl_space_split.replace(r'\\n','')\n",
    "\n",
    "wl_space_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from os import path\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# d = path.dirname(__file__)\n",
    "# 读取mask/color图片\n",
    "# nana_coloring = imread(path.join(d, r\"C:\\Users\\Shy0418\\Downloads\\worldcloud.png\"))\n",
    "nana_coloring=imread(r'C:\\Users\\Shy0418\\Downloads\\worldcloud.png')\n",
    "\n",
    "# 对分词后的文本生成词云\n",
    "my_wordcloud = WordCloud(\n",
    "                    font_path=r'C:\\Windows\\Fonts\\STKAITI.TTF',\n",
    "                    background_color = 'white', #背景颜色\n",
    "                    mask = nana_coloring,    #设置背景图片\n",
    "                    max_words = 2000,        #设置最大现实的字数\n",
    "                    stopwords = r'E:\\论文\\python代码\\停用词表\\hit_stopwords.txt',   #设置停用词\n",
    "                    max_font_size = 200,     #设置字体最大值\n",
    "                    random_state = 30,       #设置有多少种随机生成状态，即有多少种配色方案\n",
    "                    min_font_size=4, #最小自字号\n",
    "                    width=400, #默认宽度\n",
    "                    height=200, #默认高度\n",
    "                    scale=1,\n",
    "                    margin=2, #边缘\n",
    "                    # colormap='Set1',\n",
    "                    collocations=True,\n",
    "                    )\n",
    "# generate word cloud\n",
    "# create coloring from image\n",
    "image_colors = ImageColorGenerator(nana_coloring)\n",
    "my_wordcloud.generate(wl_space_split)\n",
    "# my_wordcloud.generate(zhihu_1_fenci)\n",
    "# recolor wordcloud and show\n",
    "# my_wordcloud.recolor(color_func=image_colors)\n",
    "plt.imshow(my_wordcloud)    # 显示词云图\n",
    "plt.axis(\"off\")             # 是否显示x轴、y轴下标\n",
    "plt.show()\n",
    "# save img\n",
    "my_wordcloud.to_file(\"cloudimg2.png\")\n",
    "# plt.savefig(r'词云图2.svg',format='svg',bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LDA2Vec主题分类\n",
    "from lda2vec import preprocess, Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "try:\n",
    "    import seaborn\n",
    "except:\n",
    "    pass\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "npz = np.load(open('topics.pyldavis.npz', 'r'))\n",
    "dat = {k: v for (k, v) in npz.iteritems()}\n",
    "dat['vocab'] = dat['vocab'].tolist()\n",
    "# dat['term_frequency'] = dat['term_frequency'] * 1.0 / dat['term_frequency'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "topic_to_topwords = {}\n",
    "for j, topic_to_word in enumerate(dat['topic_term_dists']):\n",
    "    top = np.argsort(topic_to_word)[::-1][:top_n]\n",
    "    msg = 'Topic %i '  % j\n",
    "    top_words = [dat['vocab'][i].strip()[:35] for i in top]\n",
    "    msg += ' '.join(top_words)\n",
    "    print msg\n",
    "    topic_to_topwords[j] = top_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "prepared_data = pyLDAvis.prepare(dat['topic_term_dists'], dat['doc_topic_dists'],\n",
    "                                 dat['doc_lengths'] * 1.0, dat['vocab'], dat['term_frequency'] * 1.0, mds='tsne')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove=('headers', 'footers', 'quotes')\n",
    "texts = fetch_20newsgroups(subset='train', remove=remove).data\n",
    "print texts[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msg = \"{weight:02d}% in topic {topic_id:02d} which has top words {text:s}\"\n",
    "for topic_id, weight in enumerate(dat['doc_topic_dists'][1]):\n",
    "    if weight > 0.01:\n",
    "        text = ', '.join(topic_to_topwords[topic_id])\n",
    "        print msg.format(topic_id=topic_id, weight=int(weight * 100.0), text=text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.bar(np.arange(20), dat['doc_topic_dists'][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print texts[51]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msg = \"{weight:02d}% in topic {topic_id:02d} which has top words {text:s}\"\n",
    "for topic_id, weight in enumerate(dat['doc_topic_dists'][51]):\n",
    "    if weight > 0.01:\n",
    "        text = ', '.join(topic_to_topwords[topic_id])\n",
    "        print msg.format(topic_id=topic_id, weight=int(weight * 100.0), text=text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.bar(np.arange(20), dat['doc_topic_dists'][51])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LDA主题分类\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PATH = \"E:/data/output.csv\"\n",
    "\n",
    "file_object2=open(PATH,encoding = 'utf-8',errors = 'ignore').read().split('\\n')  #一行行的读取内容\n",
    "data_set=[]  #建立存储分词的列表\n",
    "for i in range(len(file_object2)):\n",
    "    result=[]\n",
    "    seg_list = file_object2[i].split()\n",
    "    for w in seg_list :  #读取每一行分词\n",
    "        result.append(w)\n",
    "    data_set.append(result)\n",
    "print(data_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data_set)  # 构建词典\n",
    "corpus = [dictionary.doc2bow(text) for text in data_set]  #表示为第几个单词出现了几次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus, num_topics=10, id2word = dictionary, passes=30,random_state = 1)   #分为10个主题\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=15))  #每个主题输出15个单词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 这是确定主题数时LDA模型的构建方法，一般我们可以用指标来评估模型好坏，也可以用这些指标来确定最优主题数。一般用来评价LDA主题模型的指标有困惑度（perplexity）和主题一致性（coherence），困惑度越低或者一致性越高说明模型越好。一些研究表明perplexity并不是一个好的指标，所以一般我用coherence来评价模型并选择最优主题，但下面代码两种方法都用了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#计算困惑度\n",
    "def perplexity(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=15))\n",
    "    print(ldamodel.log_perplexity(corpus))\n",
    "    return ldamodel.log_perplexity(corpus)\n",
    "#计算coherence\n",
    "def coherence(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30,random_state = 1)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=10))\n",
    "    ldacm = CoherenceModel(model=ldamodel, texts=data_set, dictionary=dictionary, coherence='c_v')\n",
    "    print(ldacm.get_coherence())\n",
    "    return ldacm.get_coherence()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 绘制主题-conherence曲线，选择最佳主题数\n",
    "x = range(1,15)\n",
    "# z = [perplexity(i) for i in x]  #如果想用困惑度就选这个\n",
    "y = [coherence(i) for i in x]\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('主题数目')\n",
    "plt.ylabel('coherence大小')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "plt.title('主题-coherence变化情况')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 结果输出与可视化\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "import csv\n",
    "\n",
    "# 准备数据\n",
    "PATH = \"E:/data/output1.csv\"\n",
    "\n",
    "file_object2=open(PATH,encoding = 'utf-8',errors = 'ignore').read().split('\\n')  #一行行的读取内容\n",
    "data_set=[] #建立存储分词的列表\n",
    "for i in range(len(file_object2)):\n",
    "    result=[]\n",
    "    seg_list = file_object2[i].split()\n",
    "    for w in seg_list :#读取每一行分词\n",
    "        result.append(w)\n",
    "    data_set.append(result)\n",
    "\n",
    "dictionary = corpora.Dictionary(data_set)  # 构建词典\n",
    "corpus = [dictionary.doc2bow(text) for text in data_set]\n",
    "\n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes = 30,random_state=1)\n",
    "topic_list=lda.print_topics()\n",
    "print(topic_list)\n",
    "\n",
    "for i in lda.get_document_topics(corpus)[:]:\n",
    "    listj=[]\n",
    "    for j in i:\n",
    "        listj.append(j[1])\n",
    "    bz=listj.index(max(listj))\n",
    "    print(i[bz][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.save_html(data, '3topic.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
