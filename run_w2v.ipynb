{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from gensim.models import word2vec\n",
    "import jieba\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "from multistop import Stopwords\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class W2VModels(object):\n",
    "    def __init__(self, cwd, lang='english'):\n",
    "        \"\"\"\n",
    "        模型初始化设置\n",
    "        :param cwd:  当前工作路径\n",
    "        :param lang:  数据的语言\n",
    "        \"\"\"\n",
    "        self.cwd = cwd\n",
    "        self.lang = lang\n",
    "    def __preproces(self, documents):\n",
    "        \"\"\"\n",
    "        对数据进行预处理,分词、去除停用词；   可以加单词同类型合并的\n",
    "        :param documents:  文档列表\n",
    "        :return:  清洗后的文档列表\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        if self.lang=='english':\n",
    "            sw = Stopwords()\n",
    "            sw.setlang(lang=self.lang)\n",
    "            stopwords = sw.stopwords()\n",
    "            for document in documents:\n",
    "                document = document.lower()\n",
    "                document = [w for w in word_tokenize(document) if w not in stopwords]\n",
    "                docs.append(document)\n",
    "            return docs\n",
    "        elif self.lang=='chinese':\n",
    "            sw = Stopwords()\n",
    "            sw.setlang(lang=self.lang)\n",
    "            stopwords = sw.stopwords()\n",
    "            for document in documents:\n",
    "                words = jieba.lcut(document)\n",
    "                document = [w for w in words if w not in stopwords]\n",
    "                docs.append(document)\n",
    "            return docs\n",
    "        else:\n",
    "            assert 'Do not support {} language'.format(self.lang)\n",
    "\n",
    "    def train(self, documents, min_count=1):\n",
    "        \"\"\"\n",
    "        训练语料库的word2vec模型\n",
    "        :param documents:  传入的文档列表\n",
    "        :param min_count: 模型中词语最少在语料中出现min_count次\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print('数据预处理开始.......')\n",
    "        sentences = self.__preproces(documents=documents)\n",
    "        print('预处理结束...........')\n",
    "        print('Word2Vec模型训练开始......')\n",
    "\n",
    "        # sg=0代表CBOW，sg=1代表Skip-Gram\n",
    "        self.model = word2vec.Word2Vec(sentences, min_count=min_count,sample=1e-3, sg=0, window=3, hs=1,workers=multiprocessing.cpu_count(), epochs=10)\n",
    "        # print(self.model)\n",
    "        self.model.save(r\"E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\model\\word2vec.w2v\")\n",
    "        modeldir = Path(self.cwd).joinpath('model')\n",
    "        Path(self.cwd).joinpath('model').mkdir(exist_ok=True)\n",
    "        modelpath = str(Path(modeldir).joinpath('your.model'))\n",
    "        self.model.save(modelpath)\n",
    "        print('已将模型存入 {} '.format(str(modelpath)))\n",
    "\n",
    "    def __search(self, seedwords, n=50):\n",
    "\n",
    "        self.similars_candidate_idxs = [] #seedwords的候选词\n",
    "        dictionary = self.model.wv.key_to_index\n",
    "        print(dictionary)\n",
    "        self.seedidxs = [] #把word 转化为 index\n",
    "        for seed in seedwords:\n",
    "            if seed in dictionary:\n",
    "                seedidx = dictionary[seed]\n",
    "                self.seedidxs.append(seedidx)\n",
    "        print(self.seedidxs)\n",
    "        for seedidx in self.seedidxs:\n",
    "            # sims_words形如[('by', 0.99984), ('or', 0.99982), ('an', 0.99981), ('up', 0.99980)]\n",
    "            sims_words = self.model.wv.similar_by_word(seedidx, topn=n)\n",
    "            #将词语转为index存储起来\n",
    "            self.similars_candidate_idxs.extend([dictionary[sim[0]] for sim in sims_words])\n",
    "        self.similars_candidate_idxs = set(self.similars_candidate_idxs)\n",
    "\n",
    "    def find(self, seedwords, seedwordsname, topn):\n",
    "        simidx_scores = []\n",
    "        print('准备寻找每个seed在语料中所有的相似候选词')\n",
    "        self.__search(seedwords)\n",
    "        print('初步搜寻到 {} 个相似的候选词'.format(len(self.similars_candidate_idxs)))\n",
    "\n",
    "        print('计算每个候选词 与 {seedwordsname} 的相似度， 选出相似度最高的前 {topn} 个候选词'.format(seedwordsname=seedwordsname, topn=topn))\n",
    "        for idx in self.similars_candidate_idxs:\n",
    "            score = self.model.wv.n_similarity([idx], self.seedidxs)\n",
    "            # print('分数shi ：'+score)\n",
    "\n",
    "            simidx_scores.append((idx, score))\n",
    "        simidxs = [w[0] for w in sorted(simidx_scores, key=lambda k:k[1], reverse=True)]\n",
    "\n",
    "        # simwords = [str(self.model.wv.index_to_key[idx]) for idx in simidxs][:topn]\n",
    "        simwords = [str(self.model.wv.index_to_key[idx]) for idx in simidxs][:]\n",
    "\n",
    "        resultwords = []\n",
    "        resultwords.extend(seedwords)\n",
    "        resultwords.extend(simwords)\n",
    "\n",
    "        txtdir = Path(self.cwd).joinpath('candidate_words')\n",
    "        Path(self.cwd).joinpath('candidate_words').mkdir(exist_ok=True)\n",
    "        candidatetxtfile = Path(txtdir).joinpath('{}.txt'.format(seedwordsname))\n",
    "        with open(candidatetxtfile, 'w', encoding='utf-8') as f:\n",
    "            for word in resultwords:\n",
    "                f.write(word+'\\n')\n",
    "                # f.write(word+\"\\t\"+score+'\\n')\n",
    "        print('已经 【{seedwordsname} 类】 的词语筛选，并保存于 {txtfile}'.format(seedwordsname=seedwordsname, txtfile=candidatetxtfile))\n",
    "        return simwords\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = W2VModels(cwd=os.getcwd(), lang='english')\n",
    "model = W2VModels(cwd=os.getcwd(), lang='chinese')\n",
    "#df = pd.read_excel('data.xlsx')\n",
    "#model.train(documents=df['text'])\n",
    "# model.train(documents=list(open('documents.txt').readlines()))\n",
    "model.train(documents=list(open('E:\\pythonProject\\需求分析词典\\知乎问题数据集.txt',encoding='utf-8').readlines()))\n",
    "\n",
    "# model.save(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\model\\word2vec.w2v')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Existence = [w for w in open(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Existence.txt',encoding='utf-8').read().split('\\n') if w!='']\n",
    "Growth = [w for w in open(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Growth.txt',encoding='utf-8').read().split('\\n') if w!='']\n",
    "Relatedness = [w for w in open(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Relatedness.txt',encoding='utf-8').read().split('\\n') if w!='']\n",
    "# respect = [w for w in open('seeds/respect.txt').read().split('\\n') if w!='']\n",
    "# teamwork = [w for w in open('seeds/teamwork.txt').read().split('\\n') if w!='']\n",
    "\n",
    "model.find(seedwords=Existence, seedwordsname='Existence', topn=50000)\n",
    "\n",
    "model.find(seedwords=Growth, seedwordsname='Growth', topn=50000)\n",
    "\n",
    "model.find(seedwords=Relatedness, seedwordsname='Relatedness', topn=50000)\n",
    "\n",
    "# model.find(seedwords=respect, seedwordsname='respect', topn=100)\n",
    "# model.find(seedwords=teamwork, seedwordsname='teamwork', topn=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 判断是否是中文\n",
    "def is_Chinese(word):#修改过的\n",
    "    for ch in word:\n",
    "        if '\\u4e00' > ch or ch > '\\u9fff':\n",
    "            return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#仅保留中文，去除重复词语\n",
    "import pandas as pd\n",
    "Existence = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Existence.txt',encoding='utf-8',header=None,error_bad_lines=True,quoting=3)\n",
    "Existence.drop_duplicates(subset=0,keep='first',inplace=True)\n",
    "with open(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Existence_仅保留中文.txt',mode='w+',encoding='utf-8') as file:\n",
    "    for i in range(len(Existence)):\n",
    "        if is_Chinese(str(Existence.iloc[i,0])):\n",
    "            file.write(str(Existence.iloc[i,0])+'\\n')\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Growth = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Growth.txt',encoding='utf-8',header=None,error_bad_lines=True,quoting=3)\n",
    "Growth.drop_duplicates(subset=0,keep='first',inplace=True)\n",
    "with open(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Growth_仅保留中文.txt',mode='w+',encoding='utf-8') as file:\n",
    "    for i in range(len(Growth)):\n",
    "        if is_Chinese(str(Growth.iloc[i,0])):\n",
    "            file.write(str(Growth.iloc[i,0])+'\\n')\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Relatedness = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Relatedness.txt',encoding='utf-8',header=None,error_bad_lines=True,quoting=3)\n",
    "Relatedness.drop_duplicates(subset=0,keep='first',inplace=True)\n",
    "with open(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Relatedness_仅保留中文.txt',mode='w+',encoding='utf-8') as file:\n",
    "    for i in range(len(Relatedness)):\n",
    "        if is_Chinese(str(Relatedness.iloc[i,0])):\n",
    "            file.write(str(Relatedness.iloc[i,0])+'\\n')\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "data = pd.read_table(r'E:\\pythonProject\\需求分析词典\\知乎问题数据集.txt',encoding='utf-8',header=None,sep=',')\n",
    "# print(data)\n",
    "data_text=data[1].tolist()\n",
    "print(len(data_text))\n",
    "dictionary1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\dictionary.txt',encoding='utf-8',header=None)\n",
    "dictionary = dictionary1[0].tolist()\n",
    "# dictionary\n",
    "# data_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jieba.load_userdict(dictionary)\n",
    "def tokenize_zh(text):\n",
    "    seg = jieba.lcut(text)\n",
    "    words = '\\n'.join(seg)\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 慎点\n",
    "jieba.load_userdict(dictionary)\n",
    "count = 0\n",
    "with open(r'知乎问题数据集_分词结果.txt',encoding='utf-8',mode='w+') as f:\n",
    "    for i in data_text:\n",
    "        # print(i)\n",
    "        cut_text = tokenize_zh(i)\n",
    "        # print(cut_text)\n",
    "        f.write(str(cut_text))\n",
    "        # f.write('\\n')\n",
    "        print(count)\n",
    "        count+=1\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# sentence = word2vec.Text8Corpus(r'知乎问题数据集_分词结果.txt')\n",
    "# # sentence = word2vec.Text8Corpus(r'E:\\pythonProject\\制作趋势词典\\知乎问题数据集.txt')\n",
    "# model = word2vec.Word2Vec(sentence, min_count=5,sample=1e-3, sg=0, window=3, hs=1,workers=multiprocessing.cpu_count(), epochs=10)\n",
    "# print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# text = pd.read_table(r'知乎问题数据集_分词结果.txt',encoding='utf-8',header=None,sep=',',error_bad_lines=False,quoting=csv.QUOTE_NONE)\n",
    "# # text.to_csv(r'知乎问题数据集_分词结果.csv',encoding='utf-8',se)\n",
    "# print(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# text1 = text.fillna('none')\n",
    "# text1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# text2 = np.array(text1)\n",
    "# text3 = text2.reshape(-1,1)\n",
    "# text3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pd.DataFrame(text3).to_csv(r'分词.txt',encoding='utf-8',header=0,index=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 不浪费时间训练，直接加载模型。\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import multiprocessing\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s: %(message)s', level=logging.INFO) #输出日志\n",
    "# words = word2vec.Text8Corpus(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\知乎问题数据集_分词结果.txt')\n",
    "# 如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。\n",
    "# model = word2vec.Word2Vec(words,min_count=1,sample=1e-3, sg=0, window=3, hs=1, workers=multiprocessing.cpu_count(),epochs=10)\n",
    "# sg=1是skip-gram算法，对低频词敏感；默认sg=0为CBOW算法。\n",
    "# size是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。\n",
    "# window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）。\n",
    "# min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。\n",
    "# negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3。\n",
    "# hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。\n",
    "# workers控制训练的并行，此参数只有在安装了Cpython后才有效，否则只能使用单核。multiprocessing.cpu_count()==16:线程数\n",
    "# print(model)\n",
    "# model.save(r\"E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\model\\word2vec.w2v\")\n",
    "model = word2vec.Word2Vec.load(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\model\\word2vec.w2v')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 开始计算相关性分数\n",
    "import pandas as pd\n",
    "Existence_seed1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Existence.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Existence_seed1\n",
    "Existence_seed = Existence_seed1[0].to_list()\n",
    "# Existence_seed\n",
    "Existence_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Existence_仅保留中文.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Existence_candidate1\n",
    "Existence_candidate = Existence_candidate1[0].to_list()\n",
    "# Existence_candidate\n",
    "Growth_seed1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Growth.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Growth_seed1\n",
    "Growth_seed = Growth_seed1[0].to_list()\n",
    "# Existence_seed\n",
    "Growth_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Growth_仅保留中文.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Growth_candidate1\n",
    "Growth_candidate = Growth_candidate1[0].to_list()\n",
    "# Growth_candidate\n",
    "Relatedness_seed1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Relatedness.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Relatedness_seed1\n",
    "Relatedness_seed = Relatedness_seed1[0].to_list()\n",
    "# Existence_seed\n",
    "Relatedness_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Relatedness_仅保留中文.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Relatedness_candidate1\n",
    "Relatedness_candidate = Relatedness_candidate1[0].to_list()\n",
    "# Relatedness_candidate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#计算极性分数，并归类\n",
    "import os\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '8'\n",
    "import pandas as pd\n",
    "Keywords = pd.read_table(r'E:\\pythonProject\\需求分析词典\\去重后的关键词.txt',encoding='utf-8',header=None,quoting=3)\n",
    "Keywords = Keywords[0].to_list()\n",
    "len(Keywords)\n",
    "Existence_seed = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Existence.txt',encoding='utf-8',header=None,quoting=3)\n",
    "Existence_seed = Existence_seed[0].to_list()\n",
    "len(Existence_seed)\n",
    "\n",
    "\n",
    "Growth_seed = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Growth.txt',encoding='utf-8',header=None,quoting=3)\n",
    "Growth_seed = Growth_seed[0].to_list()\n",
    "len(Growth_seed)\n",
    "\n",
    "Relatedness_seed = pd.read_table(r'E:\\pythonProject\\需求分析词典\\种子词选择\\同义词扩展后的Relatedness.txt',encoding='utf-8',header=None,quoting=3)\n",
    "Relatedness_seed = Relatedness_seed[0].to_list()\n",
    "len(Relatedness_seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(r'Existence_candidate_score.txt',encoding='utf-8',mode='w+') as file_existence, open(r'Growth_candidate_score.txt',encoding='utf-8',mode='w+') as file_growth,open(r'Relatedness_candidate_score.txt',encoding='utf-8',mode='w+') as file_relatedness:\n",
    "    count1=0\n",
    "    count2=0\n",
    "    count3=0\n",
    "    for i in Keywords:\n",
    "        sum=0\n",
    "        score_existence = 0\n",
    "        score_add_existence = 0\n",
    "        score_growth=0\n",
    "        score_add_growth=0\n",
    "        score_realtedness=0\n",
    "        score_add_relatedness=0\n",
    "        for existence in Existence_seed[0:7000]:\n",
    "            try:\n",
    "                score_existence = model.wv.similarity(i,existence)\n",
    "            except KeyError:\n",
    "                score_existence=0\n",
    "            score_add_existence += score_existence\n",
    "        for growth in Growth_seed[0:7000]:\n",
    "            try:\n",
    "                score_growth = model.wv.similarity(i,growth)\n",
    "            except KeyError:\n",
    "                score_growth = 0\n",
    "            score_add_growth += score_growth\n",
    "        for relatedness in Relatedness_seed[0:7000]:\n",
    "            try:\n",
    "                score_realtedness = model.wv.similarity(i,relatedness)\n",
    "            except KeyError:\n",
    "                score_realtedness = 0\n",
    "            score_add_relatedness += score_realtedness\n",
    "        if((score_add_existence > score_add_growth) and (score_add_existence > score_add_relatedness)):\n",
    "            file_existence.write(str(i)+'\\t'+str(score_add_existence))\n",
    "            file_existence.write('\\n')\n",
    "            print('file_existence'+'\\t'+str(count1))\n",
    "            count1+=1\n",
    "        elif((score_add_growth >= score_add_existence) and (score_add_growth >= score_add_relatedness)):\n",
    "            file_growth.write(str(i)+'\\t'+str(score_add_growth))\n",
    "            file_growth.write('\\n')\n",
    "            print('file_growth'+'\\t'+str(count2))\n",
    "            count2+=1\n",
    "        elif((score_add_relatedness >= score_add_existence) and (score_add_relatedness >= score_add_existence)):\n",
    "            file_relatedness.write(str(i)+'\\t'+str(score_add_relatedness))\n",
    "            file_relatedness.write('\\n')\n",
    "            print('file_relatedness'+'\\t'+str(count3))\n",
    "            count3+=1\n",
    "        # if(count % 1000)==0:\n",
    "        #     file_existence.flush()\n",
    "        #     file_relatedness.flush()\n",
    "        #     file_growth.flush()\n",
    "file_existence.close()\n",
    "file_growth.close()\n",
    "file_relatedness.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 慎点\n",
    "with open(r'Existence_candidate_score.txt',encoding='utf-8',mode='w+') as f:\n",
    "    for i in Existence_candidate:\n",
    "        sum = 0\n",
    "        score_add_sum =0\n",
    "        score_add=0\n",
    "        score_sub1 = 0\n",
    "        score_sub1_sum =0\n",
    "        score_sub2 = 0\n",
    "        score_sub2_sum = 0\n",
    "        for j in Existence_seed:\n",
    "            try:\n",
    "                score_add = model.wv.similarity(i,j)\n",
    "            except KeyError:\n",
    "                score_add = 0\n",
    "            score_add_sum += score_add\n",
    "        for k in Growth_seed:\n",
    "            try:\n",
    "                score_sub1 = model.wv.similarity(i,k)\n",
    "            except KeyError:\n",
    "                score_sub1 = 0\n",
    "            score_sub1_sum += score_sub1\n",
    "        for l in Relatedness_seed:\n",
    "            try:\n",
    "                score_sub2 =  model.wv.similarity(i,l)\n",
    "            except KeyError:\n",
    "                score_sub2 = 0\n",
    "            score_sub2_sum += score_sub2\n",
    "        sum = (score_add_sum*2 - score_sub2_sum - score_sub1_sum)\n",
    "        print(sum)\n",
    "        f.write(str(sum))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 慎点\n",
    "with open(r'Growth_candidate_score.txt',encoding='utf-8',mode='w+') as f:\n",
    "    for i in Growth_candidate:\n",
    "        # sum = 0\n",
    "        score_add_sum =0\n",
    "        score_add=0\n",
    "        score_sub1 = 0\n",
    "        score_sub1_sum =0\n",
    "        score_sub2 = 0\n",
    "        score_sub2_sum = 0\n",
    "        for j in Growth_seed:\n",
    "            try:\n",
    "                score_add = model.wv.similarity(i,j)\n",
    "            except KeyError:\n",
    "                score_add = 0\n",
    "            score_add_sum += score_add\n",
    "        for k in Existence_seed:\n",
    "            try:\n",
    "                score_sub1 = model.wv.similarity(i,k)\n",
    "            except KeyError:\n",
    "                score_sub1 = 0\n",
    "            score_sub1_sum += score_sub1\n",
    "        for l in Relatedness_seed:\n",
    "            try:\n",
    "                score_sub2 =  model.wv.similarity(i,l)\n",
    "            except KeyError:\n",
    "                score_sub2 = 0\n",
    "            score_sub2_sum += score_sub2\n",
    "        sum = score_add_sum*2 - score_sub2_sum - score_sub1_sum\n",
    "        print(sum)\n",
    "        f.write(str(sum))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(r'Relatedness_candidate_score.txt',encoding='utf-8',mode='w+') as f:\n",
    "    for i in Relatedness_candidate:\n",
    "        # sum = 0\n",
    "        score_add_sum =0\n",
    "        score_add=0\n",
    "        score_sub1 = 0\n",
    "        score_sub1_sum =0\n",
    "        score_sub2 = 0\n",
    "        score_sub2_sum = 0\n",
    "        for j in Relatedness_seed:\n",
    "            try:\n",
    "                score_add = model.wv.similarity(i,j)\n",
    "            except KeyError:\n",
    "                score_add = 0\n",
    "            score_add_sum += score_add\n",
    "        for k in Existence_seed:\n",
    "            try:\n",
    "                score_sub1 = model.wv.similarity(i,k)\n",
    "            except KeyError:\n",
    "                score_sub1 = 0\n",
    "            score_sub1_sum += score_sub1\n",
    "        for l in Growth_seed:\n",
    "            try:\n",
    "                score_sub2 =  model.wv.similarity(i,l)\n",
    "            except KeyError:\n",
    "                score_sub2 = 0\n",
    "            score_sub2_sum += score_sub2\n",
    "        sum = score_add_sum*2 - score_sub2_sum - score_sub1_sum\n",
    "        print(sum)\n",
    "        f.write(str(sum))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    y1 = model.wv.similarity(r'全国',r'疫情')\n",
    "except KeyError:\n",
    "    y1=0\n",
    "print(y1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Existence_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Existence.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Existence_candidate1\n",
    "Existence_candidate = Existence_candidate1[0].to_list()\n",
    "Existence_candidate\n",
    "\n",
    "Growth_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Growth.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Growth_candidate1\n",
    "# Growth_candidate = Growth_candidate1[0].to_list()\n",
    "# Growth_candidate\n",
    "\n",
    "Relatedness_candidate1 = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\candidate_words\\Relatedness.txt',encoding='utf-8',header=None,quoting=3)\n",
    "# Relatedness_candidate1\n",
    "# Relatedness_candidate = Relatedness_candidate1[0].to_list()\n",
    "# Relatedness_candidate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Existence_candidate_score1=pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Existence_candidate_score.txt',encoding='utf-8',header=None)\n",
    "Existence_candidate_score = Existence_candidate_score1[0].to_list()\n",
    "Growth_candidate_score1=pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Growth_candidate_score.txt',encoding='utf-8',header=None)\n",
    "Growth_candidate_score=Growth_candidate_score1[0].to_list()\n",
    "Relatedness_candidate_score1=pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Relatedness_candidate_score.txt',encoding='utf-8',header=None)\n",
    "Relatedness_candidate_score=Relatedness_candidate_score1[0].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contact1 = pd.concat([Existence_candidate1,Existence_candidate_score1],axis=1)\n",
    "contact1.columns=['words','score']\n",
    "# 筛选出大于阈值的行\n",
    "\n",
    "# 删除word列为非字符串的行\n",
    "contact1=contact1[contact1['words'].str.isdecimal()==False]\n",
    "contact1.dropna(inplace=True)\n",
    "# 按照score的降序排序\n",
    "contact1.sort_values(by='score',ascending=False)\n",
    "# contact1\n",
    "# 去重复值\n",
    "contact1.drop_duplicates(subset=['words'], keep='first', inplace=True)\n",
    "min = contact1['score'].min() #求最小值\n",
    "min\n",
    "max = contact1['score'].max() #求最大值\n",
    "max\n",
    "#minmax归一化处理\n",
    "for i in range(len(contact1)):\n",
    "    score = (contact1.iloc[i,1] - min) / (max - min)\n",
    "    contact1.iloc[i,1] = score\n",
    "contact1.sort_values(by='score',ascending=False,inplace=True)\n",
    "contact1 = contact1.iloc[0:int(len(contact1)*threshold),0:2]\n",
    "pd.DataFrame(contact1).to_csv(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Existence_candidate_score_ultimate.txt',encoding='utf-8',header=0,index=0,sep='\\t')\n",
    "contact1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contact2 = pd.concat([Growth_candidate1,Growth_candidate_score1],axis=1)\n",
    "contact2.columns=['words','score']\n",
    "# contact2 = contact2[contact2['score'] > threshold]\n",
    "contact2=contact2[contact2['words'].str.isdecimal()==False]\n",
    "contact2.sort_values(by='score',ascending=False)\n",
    "contact2.dropna(inplace=True)\n",
    "# contact3\n",
    "contact2.drop_duplicates(subset=['words'], keep='first',inplace=True)\n",
    "contact2\n",
    "min = contact2['score'].min() #求最小值\n",
    "min\n",
    "max = contact2['score'].max() #求最大值\n",
    "max\n",
    "#minmax归一化处理\n",
    "for i in range(len(contact2)):\n",
    "    score = (contact2.iloc[i,1] - min) / (max - min)\n",
    "    contact2.iloc[i,1] = score\n",
    "contact2.sort_values(by='score',ascending=False,inplace=True)\n",
    "contact2 = contact2.iloc[0:int(len(contact2)*threshold),0:2]\n",
    "pd.DataFrame(contact2).to_csv(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Growth_candidate_score_ultimate.txt',encoding='utf-8',header=0,index=0,sep='\\t')\n",
    "contact2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = 0\n",
    "contact3 = pd.concat([Relatedness_candidate1,Relatedness_candidate_score1],axis=1)\n",
    "contact3.columns=['words','score']\n",
    "# contact3 = contact3[contact3['score']> threshold ]\n",
    "\n",
    "\n",
    "contact3=contact3[contact3['words'].str.isdecimal()==False]\n",
    "contact3.dropna(inplace=True)\n",
    "contact3.sort_values(by='score',ascending=False,inplace=True)\n",
    "# # contact5\n",
    "contact3.drop_duplicates(subset=['words'], keep='first',inplace=True)\n",
    "# contact3\n",
    "min = contact3['score'].min() #求最小值\n",
    "# min\n",
    "max = contact3['score'].max() #求最大值\n",
    "# max\n",
    "#minmax归一化处理\n",
    "for i in range(len(contact3)):\n",
    "    score = (contact3.iloc[i,1] - min) / (max - min)\n",
    "    contact3.iloc[i,1] = score\n",
    "contact3.sort_values(by='score',ascending=False,inplace=True)\n",
    "contact3 = contact3.iloc[0:int(len(contact3)*threshold),0:2]\n",
    "pd.DataFrame(contact3).to_csv(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Relatedness_candidate_score_ultimate.txt',encoding='utf-8',header=0,index=0,sep='\\t')\n",
    "contact3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SLPA实现\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on 17-11-30\n",
    "\n",
    "@summary: SLPA（Speaker-listener Label Propagation Algorithm）算法实现\n",
    "\n",
    "@author: dreamhome\n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def read_graph_from_file(path):\n",
    "    \"\"\"\n",
    "    :param path: 从文件中读取图结构\n",
    "    :return: Graph graph\n",
    "    \"\"\"\n",
    "    # 定义图\n",
    "    graph = nx.Graph()\n",
    "    # 获取边列表edges_list\n",
    "    edges_list = []\n",
    "    # 开始获取边\n",
    "    fp = open(path,encoding='utf-8')\n",
    "    edge = fp.readline().split()\n",
    "    while edge:\n",
    "        if edge[0].isdigit() and edge[1].isdigit():\n",
    "            edges_list.append((int(edge[0]), int(edge[1])))\n",
    "        edge = fp.readline().split()\n",
    "    fp.close()\n",
    "    # 为图增加边\n",
    "    graph.add_edges_from(edges_list)\n",
    "\n",
    "    # 给每个节点增加标签\n",
    "    for node, data in list(graph.nodes(data=True)):\n",
    "        data['label'] = node\n",
    "    return graph\n",
    "\n",
    "def slpa(path, threshold, iteration):\n",
    "    \"\"\"\n",
    "    slpa算法\n",
    "    :param path: 图路径\n",
    "    :param threshold:  阈值\n",
    "    :param iteration:  迭代次数\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    graph = read_graph_from_file(path)\n",
    "\n",
    "    # 节点存储器初始化\n",
    "    node_memory = []\n",
    "    for n in range(graph.number_of_nodes()):\n",
    "        node_memory.append({n+1: 1})\n",
    "\n",
    "    # 算法迭代过程\n",
    "    for t in range(iteration):\n",
    "        # 任意选择一个监听器\n",
    "        order = [x+1 for x in np.random.permutation(graph.number_of_nodes())]\n",
    "        for i in order:\n",
    "            label_list = {}\n",
    "            # 从speaker中选择一个标签传播到listener\n",
    "            for j in graph.neighbors(i):\n",
    "                sum_label = sum(node_memory[j-1].values())\n",
    "                label = node_memory[j-1].keys()[np.random.multinomial(\n",
    "                    1, [float(c) / sum_label for c in node_memory[j-1].values()]).argmax()]\n",
    "                label_list[label] = label_list.setdefault(label, 0) + 1\n",
    "            # listener选择一个最流行的标签添加到内存中\n",
    "            selected_label = max(label_list, key=label_list.get)\n",
    "            node_memory[i-1][selected_label] = node_memory[i-1].setdefault(selected_label, 0) + 1\n",
    "\n",
    "    # 根据阈值threshold删除不符合条件的标签\n",
    "    for memory in node_memory:\n",
    "        sum_label = sum(memory.values())\n",
    "        threshold_num = sum_label * threshold\n",
    "        for k, v in memory.items():\n",
    "            if v < threshold_num:\n",
    "                del memory[k]\n",
    "    # 返回划分结果\n",
    "    return node_memory\n",
    "\n",
    "path = r\"E:\\pythonProject\\pythonProject3\\Roberta_wwm_ext提取语义特征.csv\"\n",
    "print(read_graph_from_file(path))\n",
    "print(slpa(path, 0.1, 20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "class LPA():\n",
    "    '''\n",
    "    标签传播算法：传播标签来划分社区\n",
    "    算法终止条件：迭代次数超过设定值\n",
    "    self.G：图\n",
    "    return： None\n",
    "    '''\n",
    "    def __init__(self, G, iters=10):\n",
    "        self.iters = iters\n",
    "        self.G = G\n",
    "\n",
    "    def train(self):\n",
    "        max_iter_num = 0 # 迭代次数\n",
    "\n",
    "        while max_iter_num < self.iters:\n",
    "            max_iter_num += 1\n",
    "            print('迭代次数',max_iter_num)\n",
    "\n",
    "            for node in self.G:\n",
    "                count = {} # 记录邻居节点及其标签\n",
    "                for nbr in self.G.neighbors(node): # node的邻居节点\n",
    "                    label = self.G.nodes[nbr]['labels']\n",
    "                    count[label] = count.setdefault(label,0) + 1\n",
    "\n",
    "                # 找到出现次数最多的标签\n",
    "                count_items = sorted(count.items(),key=lambda x:-x[-1])\n",
    "                best_labels = [k for k,v in count_items if v == count_items[0][1]]\n",
    "                # 当多个标签频次相同时随机选取一个标签\n",
    "                label = random.sample(best_labels,1)[0]\n",
    "                self.G.nodes[node]['labels'] = label # 更新标签\n",
    "\n",
    "    def draw_picture(self):\n",
    "        # 画图\n",
    "        node_color = [float(self.G.nodes[v]['labels']) for v in self.G]\n",
    "        pos = nx.spring_layout(self.G) # 节点的布局为spring型\n",
    "        plt.figure(figsize = (8,6)) # 图片大小\n",
    "        nx.draw_networkx(self.G,pos=pos,node_color=node_color)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    G = nx.karate_club_graph() # 空手道数据集\n",
    "    # 给节点添加标签\n",
    "    print('G.nodes:')\n",
    "    print(G.nodes())\n",
    "    print('G.edges:')\n",
    "    print(G.edges())\n",
    "    print(G.graph)\n",
    "    for node in G:\n",
    "        G.add_node(node, labels = node) # 用labels的状态\n",
    "    model = LPA(G)\n",
    "    # 原始节点标签\n",
    "    model.draw_picture()\n",
    "    model.train()\n",
    "    com = set([G.nodes[node]['labels'] for node in G])\n",
    "    print('社区数量',len(com))\n",
    "    # LPA节点标签\n",
    "    model.draw_picture()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import asyn_lpa_communities as lpa\n",
    "\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# 空手道俱乐部\n",
    "G   = nx.karate_club_graph()\n",
    "com = list(lpa(G))\n",
    "print('社区数量',len(com))\n",
    "\n",
    "\n",
    "com\n",
    "[{0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 17, 19, 21, 30},\n",
    "{4, 5, 6, 10, 16},\n",
    "{14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33}]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import asyn_lpa_communities as lpa\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# 空手道俱乐部\n",
    "G   = nx.karate_club_graph()\n",
    "com = list(lpa(G))\n",
    "print('社区数量',len(com))\n",
    "print(\"点：\")\n",
    "print(G.nodes)\n",
    "print(\"边：\")\n",
    "print(G.edges)\n",
    "\n",
    "com\n",
    "[{0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 17, 19, 21, 30},\n",
    "{4, 5, 6, 10, 16},\n",
    "{14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33}]\n",
    "\n",
    "# 下面是画图\n",
    "pos = nx.spring_layout(G) # 节点的布局为spring型\n",
    "NodeId    = list(G.nodes())\n",
    "node_size = [G.degree(i)**1.2*90 for i in NodeId] # 节点大小\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8,6)) # 设置图片大小\n",
    "nx.draw(G,pos,\n",
    "        with_labels=True,\n",
    "        node_size =node_size,\n",
    "        node_color='w',\n",
    "        node_shape = '.'\n",
    "       )\n",
    "'''\n",
    "node_size表示节点大小\n",
    "node_color表示节点颜色\n",
    "with_labels=True表示节点是否带标签\n",
    "'''\n",
    "color_list = ['pink','orange','r','g','b','y','m','gray','black','c','brown']\n",
    "for i in range(len(com)):\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                           nodelist=com[i],\n",
    "                           node_color = color_list[i+2],\n",
    "                           label=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_table(r'E:\\pythonProject\\需求分析词典\\wordexpansion\\test\\词向量法\\Relatedness_candidate_score.txt',encoding='utf-8',header=None)\n",
    "data\n",
    "data[data[0]>0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485,0.456,0.406],\n",
    "        std=[0.229,0.224,0.225]\n",
    "    )\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"\")\n",
    "img.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
