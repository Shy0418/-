{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     0\n0          12 月 30 日武汉出现不明原因肺炎，目前情况如何？\n1                2019 年，你的「至暗时刻」是什么时候？\n2        大学生活有个讨厌的舍友，又碍于面子没法撕逼是怎样一种体验？\n3                      全职妈妈最终真的会成为悲剧吗？\n4             总结2019，展望2020！(附商誉地雷股名单)\n...                                ...\n598550                        为什么那么焦虑？\n598551     塞维中场：我对米兰非常感兴趣，从小就梦想为红黑军团效力\n598552  华晨宇真的没有团队吗？为什么部分人坚定地认为华晨宇没有团队？\n598553                         详细的自我介绍\n598554             如此恶心的操作，是奥克斯的常规操作吗？\n\n[598555 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12 月 30 日武汉出现不明原因肺炎，目前情况如何？</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019 年，你的「至暗时刻」是什么时候？</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>大学生活有个讨厌的舍友，又碍于面子没法撕逼是怎样一种体验？</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>全职妈妈最终真的会成为悲剧吗？</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>总结2019，展望2020！(附商誉地雷股名单)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>598550</th>\n      <td>为什么那么焦虑？</td>\n    </tr>\n    <tr>\n      <th>598551</th>\n      <td>塞维中场：我对米兰非常感兴趣，从小就梦想为红黑军团效力</td>\n    </tr>\n    <tr>\n      <th>598552</th>\n      <td>华晨宇真的没有团队吗？为什么部分人坚定地认为华晨宇没有团队？</td>\n    </tr>\n    <tr>\n      <th>598553</th>\n      <td>详细的自我介绍</td>\n    </tr>\n    <tr>\n      <th>598554</th>\n      <td>如此恶心的操作，是奥克斯的常规操作吗？</td>\n    </tr>\n  </tbody>\n</table>\n<p>598555 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhihu_question = pd.read_csv(r'E:\\pythonProject\\制作趋势词典\\知乎问题数据集.csv',encoding='utf-8',header=0,index_col=0,sep=r',',engine='python')\n",
    "zhihu_question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "598555"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zhihu_samples = zhihu_question.sample(10000)\n",
    "zhihu = zhihu_question['0'].to_list()\n",
    "len(zhihu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "12437"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "seed_words1 = pd.read_excel(r'E:\\论文\\数据\\WenXinV4.0师姐发的版本\\dic\\dictionary_all.xlsx',header=0,index_col=None)\n",
    "seed_words1\n",
    "seed_words = seed_words1['words'].to_list()\n",
    "len(seed_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Shy0418\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.675 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(r'E:\\pythonProject\\需求分析词典\\dictionary.txt')\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 这段不要随便跑\n",
    "from keybert import KeyBERT\n",
    "kw_model =  KeyBERT('all-MiniLM-L6-v2')\n",
    "count = 0\n",
    "path_keybert = r'Keybert抽取关键词.txt'\n",
    "with open(path_keybert,mode='w+',encoding='utf-8') as file_keybert:\n",
    "    for i in zhihu:\n",
    "        keywords = kw_model.extract_keywords(i,vectorizer=vectorizer,stop_words='E:\\论文\\python代码\\停用词表\\stopwords_all.txt',keyphrase_ngram_range=[1,2],use_maxsum=False,use_mmr=False,top_n = 3,seed_keywords = seed_words)\n",
    "        for j in range(len(keywords)):\n",
    "            # print(j)\n",
    "            file_keybert.write(str(keywords[j][0]+'\\t'))\n",
    "        file_keybert.write('\\n')\n",
    "        # print(keywords[0][0])\n",
    "        # print(keywords[1][0])\n",
    "        print(count)\n",
    "        count+=1\n",
    "# file_keybert.flush()\n",
    "file_keybert.close()\n",
    "time_end = datetime.datetime.now()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "           0   1    2\n0         原因  出现   武汉\n1         至暗  时刻   时候\n2       碍于面子  没法   生活\n3         成为  真的   最终\n4         名单  地雷   总结\n...      ...  ..  ...\n598550   为什么  那么   焦虑\n598551    中场  效力   从小\n598552    坚定  没有   真的\n598553  自我介绍   的   详细\n598554    恶心   的  奥克斯\n\n[598555 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>原因</td>\n      <td>出现</td>\n      <td>武汉</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>至暗</td>\n      <td>时刻</td>\n      <td>时候</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>碍于面子</td>\n      <td>没法</td>\n      <td>生活</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>成为</td>\n      <td>真的</td>\n      <td>最终</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>名单</td>\n      <td>地雷</td>\n      <td>总结</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>598550</th>\n      <td>为什么</td>\n      <td>那么</td>\n      <td>焦虑</td>\n    </tr>\n    <tr>\n      <th>598551</th>\n      <td>中场</td>\n      <td>效力</td>\n      <td>从小</td>\n    </tr>\n    <tr>\n      <th>598552</th>\n      <td>坚定</td>\n      <td>没有</td>\n      <td>真的</td>\n    </tr>\n    <tr>\n      <th>598553</th>\n      <td>自我介绍</td>\n      <td>的</td>\n      <td>详细</td>\n    </tr>\n    <tr>\n      <th>598554</th>\n      <td>恶心</td>\n      <td>的</td>\n      <td>奥克斯</td>\n    </tr>\n  </tbody>\n</table>\n<p>598555 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "data = pd.read_table(r'E:\\pythonProject\\需求分析词典\\Keybert抽取关键词.txt',encoding='utf-8',header=None,sep='\\t',quoting=csv.QUOTE_NONE)\n",
    "data1 = data.iloc[:,0:3]\n",
    "data1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1795665"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data2 = np.array(data1)\n",
    "data2\n",
    "data3 = data2.reshape(1795665,1)\n",
    "\n",
    "pd.DataFrame(data3).to_csv(r'Keybert抽取关键词处理后.txt',encoding='utf-8',header=None,index=None)\n",
    "len(data3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "3887"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "stopwords = pd.read_table(r'E:\\pythonProject\\需求分析词典\\stopwords_all.txt',encoding='utf-8',header=None,quoting=csv.QUOTE_NONE)\n",
    "stopwords1 = stopwords[0].tolist()\n",
    "len(stopwords1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 统计keybert抽取的关键词词频\n",
    "txt = open(r'E:\\pythonProject\\制作趋势词典\\Keybert抽取关键词处理后.txt', 'r',encoding='utf-8').readlines()    # 获取文本文件\n",
    "counts = {}    # 创建空字典\n",
    "for num in txt:\n",
    "    if num == ' ':    # 排除数字文本中可能出现的空格\n",
    "        continue\n",
    "    else:\n",
    "        counts[num] = counts.get(num, 0) + 1  # 统计词频并在字典中创建键值对\n",
    "items = list(counts.items())       # 将无序的字典类型转换为可排序的列表类型\n",
    "items.sort(key=lambda x: x[1], reverse=True)   # 以元素的第二列进行从大到小排序\n",
    "\n",
    "for i in range(10000):\n",
    "    num, count = items[i]\n",
    "    print(\"{:<5}:{:>5}\".format(num, count))    # 格式化输出排序结果\n",
    "    # print(num)\n",
    "    # print(type(num))\n",
    "    with open(r'Keybert提取前10000个关键词.txt',mode='a+',encoding='utf-8') as file:\n",
    "        if num not in stopwords1:\n",
    "            file.write(num)\n",
    "        else:\n",
    "            continue\n",
    "        # file.write('\\n')\n",
    "file.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TF_IDF抽取\n",
    "path_tf_idf = r'TF_IDF抽取关键词.txt'\n",
    "import jieba  # 分词器\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "jieba.load_userdict(r'E:\\pythonProject\\需求分析词典\\dictionary.txt')\n",
    "def tfidf_ana(content):\n",
    "    title_keys = jieba.analyse.extract_tags(content, topK=1000000, withWeight=False)  # topK为期望得到的关键词个数\n",
    "    with open(path_tf_idf, mode='w+',encoding='utf-8') as file:\n",
    "        for i in title_keys:\n",
    "            file.write(i + '\\n')\n",
    "        file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "answer_df = pd.read_csv('E:\\pythonProject\\需求分析词典\\知乎问题数据集.csv',encoding='utf-8')\n",
    "answer_list = answer_df['0'].to_list()\n",
    "answer_str = ''\n",
    "i=0\n",
    "for k in answer_list:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    answer_str += str(k)\n",
    "    print(k)\n",
    "data = tfidf_ana(answer_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 判断是否是中文\n",
    "def is_Chinese(word):#修改过的\n",
    "    for ch in word:\n",
    "        if '\\u4e00' > ch or ch > '\\u9fff':\n",
    "            return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "          0\n0        原因\n1        出现\n2        武汉\n3        至暗\n4        时刻\n...     ...\n177001  证券报\n177003   连忙\n177004   吩咐\n177005   起身\n177006  公元前\n\n[182112 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>原因</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>出现</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>武汉</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>至暗</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>时刻</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>177001</th>\n      <td>证券报</td>\n    </tr>\n    <tr>\n      <th>177003</th>\n      <td>连忙</td>\n    </tr>\n    <tr>\n      <th>177004</th>\n      <td>吩咐</td>\n    </tr>\n    <tr>\n      <th>177005</th>\n      <td>起身</td>\n    </tr>\n    <tr>\n      <th>177006</th>\n      <td>公元前</td>\n    </tr>\n  </tbody>\n</table>\n<p>182112 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KeyBert_keyword = pd.read_table(r'E:\\pythonProject\\需求分析词典\\Keybert抽取关键词处理后.txt',encoding='utf-8',header=None)\n",
    "Tf_IDF_keyword = pd.read_table(r'E:\\pythonProject\\需求分析词典\\TF_IDF抽取关键词.txt',encoding='utf-8',header=None)\n",
    "# # KeyBert_keyword\n",
    "# Tf_IDF_keyword\n",
    "KeyBert_Tfidf_keyworeds=pd.concat([KeyBert_keyword,Tf_IDF_keyword],axis=0)\n",
    "KeyBert_Tfidf_keyworeds.drop_duplicates(subset=0,keep='first',inplace=True)\n",
    "KeyBert_Tfidf_keyworeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 仅保留中文\n",
    "with open(r'去重后的关键词.txt',mode='w+',encoding='utf-8') as file:\n",
    "    for i in range(len(KeyBert_Tfidf_keyworeds)):\n",
    "        if(is_Chinese(str(KeyBert_Tfidf_keyworeds.iloc[i,0])) and len(str(KeyBert_Tfidf_keyworeds.iloc[i,0]))>1):\n",
    "            file.write(str(KeyBert_Tfidf_keyworeds.iloc[i,0]))\n",
    "            file.write('\\n')\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from d:\\python3.8\\lib\\site-packages\\synonyms\\data\\vocab.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [d:\\python3.8\\lib\\site-packages\\synonyms\\data\\vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\Shy0418\\AppData\\Local\\Temp\\jieba.u4ffff3a26e04aaea5c89ddb932e881e1.cache\n",
      "Loading model cost 1.406 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [d:\\python3.8\\lib\\site-packages\\synonyms\\data\\stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [d:\\python3.8\\lib\\site-packages\\synonyms\\data\\words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "# 以下测试同义词工具包\n",
    "import synonyms\n",
    "# synonyms.display('问题')\n",
    "a = synonyms.nearby('问题',10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['问题', '1.0'],\n       ['难题', '0.6956479'],\n       ['缺陷', '0.63884103'],\n       ['弊端', '0.638318'],\n       ['环境问题', '0.624778'],\n       ['结构性问题', '0.613208'],\n       ['关键问题', '0.6061078'],\n       ['弊病', '0.60467'],\n       ['症结', '0.5877637'],\n       ['风险问题', '0.5795508']], dtype='<U32')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(a).T"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
